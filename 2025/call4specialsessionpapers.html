<!DOCTYPE html>
<html lang="en">


<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv='cache-control' content='no-cache'>
  <meta http-equiv='expires' content='0'>
  <meta http-equiv='pragma' content='no-cache'>
  <title>SISAP 2025</title>
  <link rel="stylesheet" href="/2025/css/main.css">
  <link rel="canonical" href="https://www.sisap.org/2025/"> 
</head>

<body>
  <div id="page">
    <header>
      <img style="width:100%" src="/2025/images/Reykjavik_red.jpg" alt="Reykjavik" id="topbg" />
      <a href="/" id="headerlogo">
          <img src="/2025/images/sisap.png" alt="SISAP" /><br>
          <big>SISAP 2025</big><br> October 1st-3rd <br>Reykjavik, Iceland
      </a>
  </header> 

    <main id="main">
      <h1>Call for Special Session Papers</h1>

      <p>For SISAP 2025, we call for contributions for the following two special sessions:</p>

 <ul>
    <li>
        <strong>
            <a href="#irmc">IRMC: Interactive Retrieval for Multimedia Collections</a>
        </strong> (organized by Omar Shahbaz Khan, Rahel Arnold, Fabio Carrara, Ujjwal Sharma)
    </li>
    <li>
        <strong>
            <a href="#bridges">BRIDGES: Bridging Past and Present: Similarity Search for Digital Cultural Heritage and GLAM Content</a>
        </strong> (organized by Rahel Arnold, Dominik Bönisch, Fabrizio Falchi, Peter Fornaro,  Luca Rossetto, Heiko Schuldt)
    </li>
</ul>


      <p>Special session papers will supplement the regular research papers and be included in the proceedings of SISAP 2025, published by Springer in the <em>Lecture Notes in Computer Science (LNCS)</em> series.</p>

      <p>Special session submissions may include vision/position papers, evaluated based on the quality of the arguments and ideas proposed. To ensure high-quality conference papers, all submissions, including invited papers, will undergo peer review. If a session receives many high-quality submissions, some may be moved to regular sessions, and conversely, relevant accepted submissions may be moved into special sessions.</p>

      <hr>

      <h2 id="irmc">IRMC: Interactive Retrieval for Multimedia Collections</h2>

      <p> Multimedia retrieval systems aim to resolve users' search-related information needs.
        These range from simply finding a specific or a handful of media items to answering
        questions about the contents of a multimedia collection. The basic flow of such systems
        relies on similarity search based on natural language queries or content-based queries
        followed up with browsing the ranked result list. However, more often than not, the initial
        query will not find relevant items. In such cases, the following steps include rewriting,
        expanding, or refining the original query. In modern retrieval systems, interactive
        (human-in-the-loop) approaches enhance the original query or its result set. This can
        include applying filters, conducting image-to-image searches from the results, performing
        relevance feedback, and temporal querying. Additionally, these systems can internally
        optimize queries through query expansion or rewriting, with newer systems relying on
        LLMs to achieve this.

      </p><p> 

        Several advanced multimedia retrieval systems are showcased annually in the interactive
live search challenges Video Browser Showdown (VBS) at MMM and Lifelog Search
Challenge (LSC) at ICMR. These are two venues where researchers can try out new
retrieval approaches for multimedia collections with realistic tasks, where experts and
novice users operate the systems. Many lessons are learned from interactive settings,
common lessons from most systems, and system-specific ones. These lessons are
typically used to fuel further research into multimedia retrieval approaches.

</p><p> 
        Interactive retrieval approaches have two distinct applications: We can explore a diverse
multimedia collection or apply it to specialized multimedia datasets, such as medical
imaging and marine exploration. In the first case, a user wants to get an overview and
explore the collection through diverse features. When working with specialized datasets,
domain-specific content is analyzed. A collection is relatively homogeneous, which makes
the search for similarities difficult. Despite differences in individual multimedia collections,
these domains face common challenges, such as limited annotations, homogeneous data,
and the need for explainable results.

</p><p>
Beyond the challenge venues, other interesting interactive multimedia retrieval research
for common and specialized collections are published in several multimedia conferences
and journals, such as MMM, ACM ICMR, ACM MM, ACM MMSys, IEEE TMM, ACM
TOMM, MTAP, and more. We believe this session is the ideal venue to bring together the
SISAP community — specialized in similarity search, the backbone of retrieval systems —
and the interactive multimedia retrieval community to share challenges, exchange
insights, and foster research in both directions.


        </p>

        <h3>Submissions and Topics of Interest</h3>
        <p>We welcome the following types of contributions:</p>
        <ul>
            <li><strong>Vision/Position Papers:</strong> hese papers can address various aspects of interactive retrieval within specialized collections. We prefer position papers backed up by recent results, which could be already published or appear first in the paper.
            </li>
            <li><strong>Research Papers:</strong> These papers should present recent and relevant research results that may have deep implications for interactive multimedia retrieval. These papers might include system, algorithm, and user evaluation papers.</li>
        </ul>

        <strong>Topics </strong> include, but are not limited to:
        <ul>
          <li><em>Applications of interactive multimedia retrieval on specialized collections.</em></li>
          <li><em>Techniques and algorithms for similarity search in interactive multimedia retrieval.</em></li>
          <li><em>Systems, services, and implementations providing similarity search in interactive multimedia retrieval.</em></li>
          <li><em>Evaluations of similarity searches in interactive multimedia retrieval.</em></li>
      </ul>
      

        <h3>Organizers</h3>
        <ul>
            <li>Omar Shahbaz Khan, Reykjavik University, Reykjavik, Iceland</li>
            <li>Rahel Arnold, University of Basel, Basel, Switzerland</li>
            <li>Fabio Carrara, CNR-ISTI, Pisa, Italy</li>
            <li>Ujjwal Sharma, University of Amsterdam, Amsterdam, Netherlands</li>
        </ul>

        <hr>

        <h2 id="bridges">BRIDGES: Bridging Past and Present: Similarity Search for Digital Cultural Heritage and GLAM Content</h2>
      <p>With the increasing digitization of (tangible and non‐tangible) cultural heritage (CH) collections and the significant growth of born‐digital cultural heritage objects, Galleries, Libraries, Archives, and Museums (GLAM) organizations face challenges in managing, searching, and retrieving vast amounts of diverse multimedia content (images, videos and increasingly also 3D models). One challenge, especially with digitized content, is the lack of proper metadata – and often, off‐the‐shelf AI‐based tools are not very helpful since the type of data they were trained with has little or no overlap with the historical context of GLAM organizations (especially when it comes to digitized non‐tangible CH content). Traditional keyword‐based retrieval methods often fail to capture the complex semantic relationships within cultural heritage materials. Similarity search techniques, leveraging machine learning and information retrieval (and partly also aspects of computer vision) for CH and GLAM content, offer new possibilities for intuitive and effective content exploration.
      </p>

      <p>Aside from the technical aspects, similarity search in GLAM collections raises important issues of fairness, context and cultural sensitivity. The intent extends beyond identifying similar objects; it contains the imperative to ensure that these tools acknowledge and respect the significance and historical context. This is especially important when working with underrepresented histories or artefacts that are not sufficiently documented. Involving GLAM professionals in designing and refining these systems can create more meaningful and accessible ways to explore cultural heritage.</p>

      <p>This special session focuses on advances in similarity search and retrieval techniques for digital cultural heritage and GLAM content, addressing both technical innovations and real‐world applications. The session aims to bring together researchers, practitioners, and GLAM professionals to explore novel methodologies, discuss challenges, and showcase successful implementations of similarity‐based retrieval in cultural heritage contexts.
      </p>
      <p>We welcome the following types of contributions:</p>
        <ul>
            <li><strong>Research Papers</strong>: These contributions should summarize recent and relevant research findings on similarity search in CH and GLAM content.
            </li>
            <li><strong>Application Papers</strong>: These contributions should showcase practical implementations, case studies, or real‐world deployments of interactive multimedia retrieval systems, particularly within GLAM institutions and digital cultural heritage collections. Submissions should highlight challenges, solutions, and lessons learned from applying similarity search techniques in practice
 
            </li>
            <li><strong>Position Papers</strong>: These contributions should present forward‐looking perspectives on applying similarity search in CH and GLAM contexts. The focus is on novel approaches, emerging trends, and underexplored challenges in the field. We welcome contributions that propose innovative frameworks, interdisciplinary methodologies, or new application domains, particularly those that have not been addressed in sufficient depth or breadth so far.

   </ul>
   We solicit contributions on (but not limited to) the following <strong> topics </strong>:
   <ul>
    <li><em>AI and Machine Learning for Similarity Search in CH and GLAM content:</em> Deep learning and traditional approaches for content‐based retrieval in heritage collections.</li>
    <li><em>Multimodal and Cross‐Modal Retrieval in CH and GLAM content:</em> Searching across textual, visual, and audio content in cultural heritage datasets.</li>
    <li><em>Metadata Generation:</em> Novel approaches to generate semantic metadata on digitized CH and GLAM content, especially for artefacts representing non‐tangible content.</li>
    <li><em>Semantic and Knowledge‐Based Approaches:</em> Ontologies, knowledge graphs, and linked data for enhancing similarity search.</li>
    <li><em>Large‐Scale and Scalable Retrieval Systems:</em> Indexing and efficient search in massive digital heritage archives.</li>
    <li><em>User‐Centric and Explainable Retrieval:</em> Interactive search interfaces, user experience, and explainability in similarity‐based retrieval systems.</li>
    <li><em>Applications in GLAM Institutions:</em> Case studies, success stories, and challenges faced by museums, libraries, and archives in implementing similarity search solutions.</li>
    <li><em>Evaluation of Retrieval in CH and GLAM organizations:</em> User studies and approaches for assessing the effectiveness of retrieval in CH and GLAM organizations.</li>
</ul>
<h3>Organizers</h3>
<ul>
  <li>Rahel Arnold, Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland</li>
  <li>Dominik Bönisch, MIREVI, University of Applied Sciences Düsseldorf, Duesseldorf, Germany</li>
  <li>Fabrizio Falchi, CNR-ISTI, Italy</li>
  <li>Peter Fornaro, Digital Humanities, University of Basel, Basel, Switzerland</li>
  <li>Luca Rossetto, Dublin City University, Ireland</li>
  <li>Heiko Schuldt, Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland</li>
</ul>

    </main>
    <nav id="sidebar"></nav>
    <aside id="aside-container"></aside>
    <footer id="footer-container"></footer>
  </div>
  <script>
    // Load the  from the common html files
    function loadHTML(id, url) {
        fetch(url)
            .then(response => response.text())
            .then(data => {
                document.getElementById(id).innerHTML = data; 
            })
            .catch(error => console.error('Error loading the HTML file: ', error));
    }

    // Load HTML files
    window.onload = function() {
        // loadHTML("header-container", "/2025/includes/header.html");
        loadHTML("footer-container", "/2025/includes/footer.html");
        loadHTML("aside-container", "/2025/includes/aside.html");
        loadHTML("sidebar", "/2025/includes/nav.html");
    }
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Towards a Learned Index Structure for Approximate Nearest Neighbor Search Query Processing</title>
<link rel="stylesheet" href="/2021/css/main.css">
<link rel="canonical" href="https://www.sisap.org/2021/poster/15.html">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="page">
<header>
<img src="/2021/images/header-cherry.jpg" alt="" id="topbg" />
<a href="/2021/" id="headerlogo"><img src="/2021/images/sisap.png" alt="SISAP" /><br>
<big>SISAP 2021</big><br> Sept 29 &ndash; Oct 01<br> Dortmund, Germany</a>
</header>
<main id="main">
<h1>Towards a Learned Index Structure for Approximate Nearest Neighbor Search Query Processing</h1>
<p><strong>Maximilian Hünemörder, Peer Kröger and Matthias Renz</strong></p>
<p>In our short paper, we outline the idea of applying the concept of a learned index structure to approximate nearest neighbor query processing. We discuss different data partitioning approaches and show how the task of identifying the disc pages of potential hits for a given query can be solved by a predictive machine learning model.</p>
<h2><strong>Method</strong></h2>
<hr />
<p><img src="15_kmeans-lis.png" alt="Method" /></p>
<p>As seen above a data set \(\mathcal{D}\) (blue dots) is supposed to be distributed over a set \(\mathcal{P}\) of \(p\) pages depending on the fixed capacity \(c\) of the datapages so that a Classifier can learn this partioning.</p>
<p>Points assigned to a cluster \(C_i (1\leq i \leq k)\) (orange, red and yellow dots) created by a partioning or clustering algorithm are mapped to page \(P_i\in\mathcal{P}\).</p>
<p>The classifier is then trained on the pseudolabels gained from the clustering algorithm and can then predict the page \(P\in\mathcal{P}\) where the query object \(q\) should be be placed on with learned function \(M : \mathcal{F} \rightarrow \mathcal{P}\).</p>
<p>The page \(P\) can be loaded into main memory and the nearest neighbors of \(q\) among all objects stored on \(P\) can be determined and returned as (approximate) result.</p>
<h2><strong>Experiments</strong></h2>
<hr />
<p>In order to get a first impression of the proposed <strong>Learned Index Structure</strong> for <strong>Approximate Nearest Neighbor</strong> query processing, we used two synthetic data sets of 20-dimensional clustered and non-clustered data.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="15_clustered_example.png" width="49%" alt="Clustered Dataset" /><img src="15_uniform_example.png" width="49%" alt="Uniform Dataset" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Single Examples of the Synthetic datasets. Clustered and Uniform.</em></td>
    </tr>
  </tbody>
</table>
<p>Additionally, we used a low dimensional embedding of the popular <strong>MNIST</strong> data set generated by a fully connected Autoencoder (AE).</p>
<p>For the <strong>partitioning</strong> step, we used k-means and the leaf nodes of a kd-tree.</p>
<p>As predictive models, we used:</p>
<ul>
  <li>A <em>Base Model</em>, where we assign each query object to its closest centroid of the corresponding partition</li>
  <li>Naive Bayes</li>
  <li>Decision Tree and Random Forest</li>
  <li>Support Vector Machine (SVM) with a linear and an rbf kernel</li>
  <li>A simple dense multi-layer perceptron (MLP).</li>
</ul>
<p>For these preliminary experiments we did not perform hyper-parameter tuning but used reasonable default parameters. The <strong>AutoEncoder</strong> for the MNIST data set has only one single linear layer that maps the flattened images (784 dimensional array) to a latent space vector of 32 dimensions (using Leaky ReLU as activation).</p>
<p>For Evaluation we used:</p>
<ul>
  <li><strong>validation accuracy</strong> of the classifier with a single train-validation split
  \[ ACC_{Val} = \frac{TP + TN}{P + N} \]</li>
  <li><strong>test (query) accuracy</strong> of the index with a set of perviously unseen query points \(Q\) originating from the same data process
  \[ ACC_{Test} = \frac{1}{|Q|} \cdot \sum_ {q \in Q} \begin{cases} 1, &amp; \text{if } |NN_{D} - NN_{P_{M(q)}} | = 0 \ 0, &amp; \text{otherwise} \end{cases} \]</li>
</ul>
<h2><strong>Results</strong></h2>
<p>We analysed the relationship between the test accuracy and the number of samples and number of partitions, i.e., data pages over several runs and report the mean evalution measures. In all runs, we kept the capacity of pages fixed at 10.000 but changed the number of data points accordingly.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="15_kmeans_clustered_test.png" alt="Results" width="49%" /> <img src="15_kmeans_clustered_valid.png" alt="Results" width="49%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Test and Validation Accuracy for k-means on the Clustered Datasets</em></td>
    </tr>
  </tbody>
</table>
<p>It is interesting to note that for most models the validation error remains better than the test accuracy, i.e. even though, the mapping is learned well, the true NNs for the query objects are approximated not quite as well. In these cases, the partitioning model seems to not optimally fit the real data distribution and therefore even with a perfect predictive model some queries can be placed in an unsuitable data page.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="15_kdtree_clustered_test.png" alt="Results" width="49%" /> <img src="15_kdtree_clustered_valid.png" alt="Results" width="49%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Test and Validation Accuracy for the leaves of a kd-tree on the Clustered Datasets</em></td>
    </tr>
  </tbody>
</table>
<p>This is also reflected inthe fact that the kd-tree partitioning performs even worse in terms of test accuracy, since  the clustered dataset was created in a way that favours k-means.</p>
<p>We can also observe that the Decision Tree classifier shows perfect validation accuracy for the kd-tree partitioning, while showing the worst performance for k-means. This suggests that choosing a fitting pair of prediction and partitioning algorithm is vital to at least result in a high validation accuracy. These observations are further confirmed by the non-clustered datasets (these results and our implementation can be found in our <a href="https://github.com/huenemoerder/kmean-lis">git repository</a>). Additionally, this is further reflected in our results on MNIST in our paper, where the test accuracies for the kdtree paritioning are significantly worse than the ones for k-means. Generally further experiments and benchmarking are obviously necessary to obtain more significant results.</p>
<h2><strong>Conclusion</strong></h2>
<p>The results are generally promising for synthetic data such that  we think it  is worth  putting more  future focus on LIS for ANN query processing.</p>
<p>For example, exploring new ways for data partitioning including a more thorough evaluation of different existing partitioning schemes could be interesting. Also, understanding  the  relationship  between  data  characteristics,  properties  of  the  partitioning, and the accuracy of different predictive models could be a promising research direction, that may lead to approaches that better integrate partitioning and learning. Additionally,exploring post-processing methods to increase accuracy, e.g. use additional information from training as well as from the partitioning like distance bounds would be helpful.Last not least, the application of LIS to other types of similarity queries is still an open research question.</p>
<hr />
<p><a href="https://link.springer.com/chapter/10.1007/978-3-030-89657-7_8">Paper</a></p>
<p><a href="https://www.youtube.com/watch?v=uDxhYT6rS2w&amp;list=PLU6sx10A4i3YMTkRclcn8HEXPYWZrLJdo">Video Presentation</a></p>
</main>
<nav id="sidebar">
<h2><a class="top" href="/2021/">SISAP 2021</a></h2>
<ul>
<li><a href="/2021/">Home</a></li>
<li><a href="/2021/callforpapers.html">Call for Research Contributions</a></li>
<li><a href="/2021/specialsessions.html">Call for Special Session Submissions</a></li>
<li><a href="/2021/doctoralsymposium.html">Call for Doctoral Symposium Papers</a></li>
<li><a href="/2021/organization.html">Organization</a></li>
<li><a href="/2021/pc.html">Program Committee</a></li>
</ul>
<h2>Contributions</h2>
<ul>
<li><a href="/2021/guidelines.html">Submission Guidelines</a></li>
<li><a href="/2021/importantdates.html">Important Dates</a></li>
<li><a href="/2021/papers.html">Accepted Papers</a></li>
<li><a href="/2021/camerareadyversion.html">Camera Ready Version</a></li>
</ul>
<h2>Attendance</h2>
<ul>
<li><a href="/2021/registration.html">Registration</a></li>
<li><a href="/2021/conferenceoutline.html">Conference Outline</a></li>
<li><a href="/2021/conferenceprogram.html">Conference Program</a></li>
</ul>
<h2>Previous Conferences</h2>
<ul>
<li><a href="http://www.sisap.org/2020">SISAP 2020</a></li>
<li><a href="http://www.sisap.org/2019">SISAP 2019</a></li>
<li><a href="http://www.sisap.org/2018">SISAP 2018</a></li>
<li><a href="http://www.sisap.org/2017">SISAP 2017</a></li>
<li><a href="http://www.sisap.org/2016">SISAP 2016</a></li>
<li><a href="http://www.sisap.org/">Earlier SISAPs</a></li>
</ul>
</nav>
<aside>
<h2 style="color:#900">Virtual or Hybrid?</h2>
<p><b>Virtual</b></p>
<p>The conference will be entirely <em>virtual</em>.
Current development in Europe is too dynamic.</p>
<h2>News</h2>
<p><a href="https://link.springer.com/book/10.1007/978-3-030-89657-7">Proceedings</a></p>
<p><a href="https://www.youtube.com/playlist?list=PLU6sx10A4i3YMTkRclcn8HEXPYWZrLJdo">SISAP video presentations</a></p>
<hr>
<p><a href="/2021/awards.html">Best paper awards</a></p>
<hr>
<p><a href="/2021/conferenceprogram.html">Conference program</a></p>
<hr>
<p><a href="/2021/registration.html">Free registration</a></p>
<hr>
<!--<p>Please <b>upload your video recordings</b>!</p>
<hr>
<p><a href="/2021/camerareadyversion.html">Camera-ready instructions</a>
were mailed to corresponding authors.</p>
<hr> -->
<a href="/2021/papers.html">Accepted Papers</a>
<hr>
<!--<p>Notifications: July&nbsp;24,&nbsp;2021</p>
<hr>-->
<p>May 31 2021: SISAP is now a <a href="http://portal.core.edu.au/conf-ranks/2240/">CORE&nbsp;Rank&nbsp;B</a> conference.</p>
<hr>
<p>The <a href="/2021/specialsessions.html">call for special sessions submissions</a> is posted</p>
<hr>
<p>The <a href="/2021/callforpapers.html">call for papers</a> is posted on the web site</p>
<h3>Submission deadline:</h3>
<p><a href="/2021/importantdates.html">June 14, 2021 AoE (<b>closed</b>)</a></p>
</aside>
<footer>
<a href="http://www.springer.com/computer/lncs"><img src="/2021/images/LNCS.jpg"/></a>
<a href="http://www.journals.elsevier.com/information-systems/"><img src="/2021/images/is.png" /></a>
<!--
<a href="https://www.google.com/"><img src="images/google.png"/></a>
-->
<hr>
<div id="credits">
Image credit:
Mathetower with cherry blossoms is derived work based on a
<a href="https://commons.wikimedia.org/wiki/File:Mathetower_TU_Dortmund.jpg">CC-BY-SA 3.0 image by Sonja Ludwig</a>
<br>
Legal contact for 2021 only: <a href="https://www-ai.cs.uni-dortmund.de/PERSONAL/schubert.html">Erich Schubert</a>.
</div>
</footer>
</div>
<script src="/2021/js/poster.js"></script>
</body>
</html>
